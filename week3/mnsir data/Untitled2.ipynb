{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ed5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdf1fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:14<00:00, 685850.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 111480.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:06<00:00, 243550.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 175545.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define transformation pipeline for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image or numpy.ndarray to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] range (mean=0.5, std=0.5 for grayscale)\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Load MNIST dataset and create data loaders\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define classes in MNIST dataset\n",
    "classes = tuple(str(i) for i in range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3370b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def display_images(images, labels):\n",
    "    for j in range(len(images)):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        image = images[j] / 2 + 0.5  # Unnormalize\n",
    "        npimg = image.squeeze().numpy()\n",
    "        plt.imshow(npimg, cmap='gray')\n",
    "        plt.title(f'Ground Truth: {classes[labels[j]]}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e858417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 input channel (grayscale), 6 output channels, 5x5 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling with 2x2 kernel and stride\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # 6 input channels, 16 output channels, 5x5 kernel\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Fully connected layer: 16 * 4 * 4 input features, 120 output features\n",
    "        self.fc2 = nn.Linear(120, 84)  # Fully connected layer: 120 input features, 84 output features\n",
    "        self.fc3 = nn.Linear(84, 10)  # Fully connected layer: 84 input features, 10 output features (classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))  # Convolution -> ReLU -> Max pooling\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))  # Convolution -> ReLU -> Max pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten 2D features to 1D\n",
    "        x = nn.functional.relu(self.fc1(x))  # Fully connected -> ReLU\n",
    "        x = nn.functional.relu(self.fc2(x))  # Fully connected -> ReLU\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "cnn_model = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781cd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feed-forward Neural Network (FNN) model\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Fully connected layer: 28 * 28 input features, 128 output features\n",
    "        self.fc2 = nn.Linear(128, 64)  # Fully connected layer: 128 input features, 64 output features\n",
    "        self.fc3 = nn.Linear(64, 10)  # Fully connected layer: 64 input features, 10 output features (classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten 2D features to 1D\n",
    "        x = nn.functional.relu(self.fc1(x))  # Fully connected -> ReLU\n",
    "        x = nn.functional.relu(self.fc2(x))  # Fully connected -> ReLU\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the FNN model\n",
    "fnn_model = FNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28d67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer for CNN\n",
    "cnn_criterion = nn.CrossEntropyLoss()  # Cross entropy loss function for multi-class classification\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)  # Adam optimizer with learning rate\n",
    "\n",
    "# Define loss function and optimizer for FNN\n",
    "fnn_criterion = nn.CrossEntropyLoss()  # Cross entropy loss function for multi-class classification\n",
    "fnn_optimizer = optim.Adam(fnn_model.parameters(), lr=0.001)  # Adam optimizer with learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7681faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN [1,   100] loss: 1.149\n",
      "CNN [1,   200] loss: 0.342\n",
      "CNN [1,   300] loss: 0.218\n",
      "CNN [1,   400] loss: 0.162\n",
      "CNN [1,   500] loss: 0.137\n",
      "CNN [1,   600] loss: 0.132\n",
      "CNN [1,   700] loss: 0.097\n",
      "CNN [1,   800] loss: 0.095\n",
      "CNN [1,   900] loss: 0.090\n",
      "CNN [2,   100] loss: 0.078\n",
      "CNN [2,   200] loss: 0.072\n",
      "CNN [2,   300] loss: 0.080\n",
      "CNN [2,   400] loss: 0.072\n",
      "CNN [2,   500] loss: 0.069\n",
      "CNN [2,   600] loss: 0.062\n",
      "CNN [2,   700] loss: 0.068\n",
      "CNN [2,   800] loss: 0.055\n",
      "CNN [2,   900] loss: 0.059\n",
      "CNN [3,   100] loss: 0.047\n",
      "CNN [3,   200] loss: 0.050\n",
      "CNN [3,   300] loss: 0.045\n",
      "CNN [3,   400] loss: 0.049\n",
      "CNN [3,   500] loss: 0.051\n",
      "CNN [3,   600] loss: 0.050\n",
      "CNN [3,   700] loss: 0.050\n",
      "CNN [3,   800] loss: 0.047\n",
      "CNN [3,   900] loss: 0.046\n",
      "CNN [4,   100] loss: 0.039\n",
      "CNN [4,   200] loss: 0.035\n",
      "CNN [4,   300] loss: 0.036\n",
      "CNN [4,   400] loss: 0.041\n",
      "CNN [4,   500] loss: 0.034\n",
      "CNN [4,   600] loss: 0.035\n",
      "CNN [4,   700] loss: 0.044\n",
      "CNN [4,   800] loss: 0.040\n",
      "CNN [4,   900] loss: 0.041\n",
      "CNN [5,   100] loss: 0.028\n",
      "CNN [5,   200] loss: 0.036\n",
      "CNN [5,   300] loss: 0.026\n",
      "CNN [5,   400] loss: 0.031\n",
      "CNN [5,   500] loss: 0.030\n",
      "CNN [5,   600] loss: 0.033\n",
      "CNN [5,   700] loss: 0.034\n",
      "CNN [5,   800] loss: 0.031\n",
      "CNN [5,   900] loss: 0.030\n",
      "Finished Training CNN\n"
     ]
    }
   ],
   "source": [
    "# Training loop for CNN\n",
    "def train_cnn(model, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'CNN [{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training CNN')\n",
    "\n",
    "# Train the CNN model\n",
    "train_cnn(cnn_model, cnn_criterion, cnn_optimizer, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def1377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Accuracy on the test dataset: 98.83 %\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate accuracy for CNN\n",
    "def test_cnn(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'CNN Accuracy on the test dataset: {100 * correct / total:.2f} %')\n",
    "\n",
    "# Test CNN model\n",
    "test_cnn(cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89cc1089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Accuracy on the test dataset: 8.73 %\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate accuracy for FNN\n",
    "def test_fnn(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.view(images.size(0), -1)  # Flatten input for FNN\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'FNN Accuracy on the test dataset: {100 * correct / total:.2f} %')\n",
    "\n",
    "# Test FNN model\n",
    "test_fnn(fnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63924f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
